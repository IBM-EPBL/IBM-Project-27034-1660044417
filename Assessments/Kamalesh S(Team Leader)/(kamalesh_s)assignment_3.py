# -*- coding: utf-8 -*-
"""(Kamalesh_S)Assignment 3.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1i4iwM2uLxQ9RYFnEQ2wUd2Fyu_g9-V5O

# Importing the libaries
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

"""## 1. Downloading the dataset
## 2. Loading the dataset into the tool
"""

#Importing the dataset
data = pd.read_csv('/content/abalone.csv')

#Printing the top first data lables and values
data.head()

"""## 3. visualization
### Univariate Analysis
"""

plt.hist(data['Sex'])

"""### Bivariate Analysis"""

plt.scatter(data['Shell weight'],data['Rings'])

"""### Multivariate Analysis"""

sns.pairplot(data,hue='Sex')

"""## 4. Descriptive Statistics """

data.describe()

"""## 5. Checking for missing values


"""

data.isna().sum()

"""The above result shows there are no null values in the lables of the data

##6. Finding the outliers and replacing them
"""

data.plot(kind="box",subplots=True,layout=(7,4),figsize=(25,30));

"""**Quantiles** are cut points dividing the range of a probability distribution into continuous intervals with equal probabilities, or dividing the observations in a sample"""

qnt=data.quantile(q=[0.25,0.75])
qnt

iqr=qnt.loc[0.75]-qnt.loc[0.25] 
iqr

lower=qnt.loc[0.25]-(1.5*iqr)
lower

upper=qnt.loc[0.75]+(1.5*iqr)
upper

data.mean()

"""#### Length"""

data['Length']=np.where(data['Length']< 0.22,0.53,data['Length'])

sns.boxplot(x=data['Length'])

"""#### Diameter"""

data['Diameter']=np.where(data['Diameter']< 0.155,0.407,data['Diameter'])

sns.boxplot(x=data['Diameter'])

"""#### Height"""

data['Height']=np.where(data['Height']< 0.04,0.14,data['Height'])

sns.boxplot(x=data['Height'])

data['Height']=np.where(data['Height']>0.24,0.14,data['Height'])
sns.boxplot(x=data['Height'])

"""#### Whole weight"""

data['Whole weight']=np.where(data['Whole weight']>2.18,0.83,data['Whole weight'])
sns.boxplot(x=data['Whole weight'])

"""#### Viscera weight"""

data['Viscera weight']=np.where(data['Viscera weight']>0.478,0.18,data['Viscera weight'])
sns.boxplot(x=data['Viscera weight'])

"""#### Shell weight"""

data['Shell weight']=np.where(data['Shell weight']>0.61,0.238831,data['Shell weight'])
sns.boxplot(x=data['Shell weight'])

"""#### Rings"""

data['Rings']=np.where(data['Rings']>15.5,9.933684,data['Rings'])
sns.boxplot(x=data['Rings'])

data['Rings']=np.where(data['Rings']<3.5,9.933684,data['Rings'])
sns.boxplot(x=data['Rings'])

data.plot(kind="box",subplots=True,layout=(7,4),figsize=(25,30))

"""## 7. Check for Categorical Columns and perform encoding"""

data.info()

data['Sex'].unique()

df=pd.get_dummies(data,columns=['Sex'])
df.head()

"""##8. Split the data into dependent and independent variables

At first, the data is splitted as dependent variable initially
"""

y = df['Rings'].values
y

"""Here then the data is splitted as independent variables"""

x=df.drop(columns=['Rings'],axis=1).values
x

"""## 9. Scale the independent variables"""

from sklearn.preprocessing import scale

"""Scaling the independent variables"""

x = scale(x)
x

"""##10. Split the data into training and testing

Spitting is done in the data for to terminate the overfitting instance
"""

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=0)

"""##11. Build the model"""

from sklearn.linear_model import LinearRegression

LR_data = LinearRegression()

"""##12. Train the model"""

LR_data.fit(x_train,y_train)

"""##13. Test the model"""

y_predict = LR_data.predict(x_test)
y_predict

"""##14. Measure the performance using Metrics
Here, we are measuring the performance using one of the regression metrics such as r2_score

We are performing the metrics analysis using the following regression metrics
- R2 Score 
- Mean Absolute Percentage Error
- Mean Poisson Deviance
- Mean Pinboll Loss

#### R2 Score
"""

from sklearn.metrics import r2_score
score = r2_score(y_test,y_pred)
score

"""From the above R2 Score, it is less than 1.0 which means that all predictions matches the expected values exactly.

#### Mean Absolute Percentage Error
"""

from sklearn.metrics import mean_absolute_percentage_error
MSE = mean_absolute_percentage_error(y_test,y_pred)
MSE

"""Also by checking the performance using Mean Absolute Percentage Error above,we can see that the prediction is under 1.0 value. Hence, the prediction succeeds

#### Mean Poisson Deviance
"""

from sklearn.metrics import mean_poisson_deviance
MPD = mean_poisson_deviance(y_test,y_pred)
MPD

"""It results under 1.0 for Mean Poisson Deviance also as accurate at prediction

#### Mean Pinball Loss
"""

from sklearn.metrics import mean_pinball_loss
pinball_score = mean_pinball_loss(y_test,y_pred)
pinball_score

"""It is good for mean_pinball_loss also in prediction"""

df.head()

LR_data.predict([[0.455,0.365,0.095,0.5140,0.2245,0.1010,0.150,0,0,1]])